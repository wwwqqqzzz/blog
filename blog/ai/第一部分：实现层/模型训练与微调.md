---
cssclasses:
  - ai
  - 实现层
tags:
  - ai学习
  - fine-tuning
  - lora
  - rlhf
  - dpo
title: 模型训练与微调 - 定制你的专属模型
date: 2026-02-04
authors:
  - wqz
description: 当 Prompt 工程遇到天花板，如何通过 Fine-tuning 让模型真正听懂你的话？LoRA 和 RLHF 又是怎么回事？
collection: 第二部分：实现层
slug: model-training-finetuning
collection_order: 1
---

# 模型训练与微调 - 定制你的专属模型

:::info 现在的你位置
我们刚刚完成了 **Layer 1: 基础理论**，搞懂了模型在大脑里是怎么“思考”的。
现在进入 **Layer 2: 实现层**。我们要聊聊工程师最关心的问题：**既然开源模型都这么强了，我还有必要自己训练吗？如果要有，怎么训才最省钱？**
:::

---

## 1. 核心决策：Prompt, RAG 还是 Fine-tuning?

做 AI 应用时，这是第一个要做的架构决策。很多新手会陷入误区：觉得微调出来的模型一定比 Prompt 强。

其实它们解决的是完全不同的问题：

| 方法                   | 本质         | 解决的问题                                                      | 缺点                                         | 类比                                     |
| :--------------------- | :----------- | :-------------------------------------------------------------- | :------------------------------------------- | :--------------------------------------- |
| **Prompt Engineering** | 提示词工程   | **短期记忆**。教模型理解你当下的任务。                          | 上下文有限，费 Token，每次都要重复教。       | 给实习生写一张便签条，告诉他今天干什么。 |
| **RAG**                | 检索增强生成 | **外挂知识库**。给模型补充它不知道的私有数据。                  | 只能通过阅读理解回答，并没有改变模型的能力。 | 给实习生一本公司手册，让他照着查。       |
| **Fine-tuning**        | 微调         | **肌肉记忆/内化**。改变模型内部的参数，让它获得新的技能或风格。 | 成本高，需要高质量数据，可能导致灾难性遗忘。 | 送实习生去培训班进修一个月。             |

### ✅ 什么时候必须微调？

1.  **为了“格式”**：你希望模型输出非常特殊的 JSON 结构，Prompt 总是出错，微调一下就老实了。
2.  **为了“风格”**：你希望模型说话像“鲁迅”或者“马斯克”，Prompt 很难模仿到位，微调可以学会这种语气。
3.  **为了“能力”**：比如让一个通用模型学会看懂“医疗心电图数据”或者“法律文书”，这是知识注入。
4.  **为了“缩减成本”**：把一个 GPT-4 级别的 Prompt 效果，蒸馏到一个 7B 的小模型上，跑在本地。

---

## 2. 训练的三个阶段：从“读万卷书”到“懂人情世故”

现在的 LLM（大语言模型）不是一次练成的，它经历了三个阶段：

### 第一阶段: Pre-training (预训练) - 获得“通识”

- **做什么**：把互联网上几万亿的文字（Wiki, GitHub, 新闻）喂给它，让它做“完形填空”。
- **状态**：**Base Model (基座模型)**。
  - 它博学多才，但不懂交互。
  - 你问它：“怎么做番茄炒蛋？”
  - 它可能接着写：“...的做法是很多家庭的难题。番茄炒蛋的历史可以追溯到...” (它以为在续写百科全书)。

### 第二阶段: SFT (Supervised Fine-Tuning, 指令微调) - 学会“对话”

- **做什么**：给它看高质量的问答对（Q&A）。
  - _User: 把这句话翻译成英文。_
  - _Assistant: 好的，Here you go._
- **状态**：**Chat Model (对话模型)**。
  - 它终于听懂指令了，知道该怎么伺候人类了。
  - 我们常说的“Fine-tuning”通常指这一步。

:::info 对比一下 Prompt 和 Fine-tuning 的区别
**Prompt (提示词)**:

```text
你是一个资深的翻译官。请把下面这句话翻译成英文，要求信达雅：
"今天天气不错。"
```

**Fine-tuning (微调后)**:
不需要复杂的 Prompt，直接给数据：

```text
User: 翻译：今天天气不错。
Assistant: The weather is lovely today.
```

:::

### 第三阶段: Alignment (RLHF / DPO, 对齐) - 树立“价值观”

- **做什么**：让它知道什么回答是好的，什么是不好的（避免有害内容、偏见、暴力）。
- **状态**：**Safe & Helpful Model**。
  - 这是商业模型（如 ChatGPT）必不可少的一步。

---

## 3. LoRA: 穷人的法拉利 (Low-Rank Adaptation)

全量微调（Full Fine-tuning）一个 70B 的模型，可能需要几十张 A100 显卡，成本极高。
因为你要更新模型里的**所有参数**。

微软提出的 **LoRA** 改变了游戏规则。它是目前最主流的微调技术。

### 3.1 核心直觉

想象一个训练好的大模型是一块巨大的**冰块**（参数冻结，Fixed）。
我们要在上面雕刻新的花纹。

LoRA 说：**别动那个大冰块！我们在旁边贴两张小贴纸（矩阵）这就好了。**

它在原本的矩阵 $W$ 旁边，挂了两个极小的矩阵 $A$ 和 $B$ (Low-Rank Matrices)，只训练这两个小矩阵。

:::info LoRA 公式
$$ W' = W + \Delta W = W + A \times B $$

- $W$：冻结的大模型权重（几百 GB）。
- $A, B$：我们训练的小插件（几百 MB）。
- 训练时，只更新 $A$ 和 $B$。
- 推理时，把 $A \times B$ 加回 $W$ 里去，完全不增加推理延迟！
  :::

### 3.2 为什么 LoRA 伟大？

1.  **省显存**：原本需要 4 张卡，现在 1 张常用的 RTX 4090 都能跑。
2.  **可插拔**：你可以训练一个“写作 LoRA”，一个“编程 LoRA”。用的时候像换皮肤一样挂载上去，主模型不用变。
3.  **效果好**：效果几乎能达到全量微调的 95% 以上。

---

## 4. RLHF vs DPO: 怎么教 AI 懂礼貌？

你可能听过 OpenAI 的 **RLHF (Reinforcement Learning from Human Feedback)**。它是 ChatGPT 成功的秘诀之一。

### RLHF 的流程 (传统方法)

1.  **SFT 模型**：先有个能说话的模型。
2.  **Reward Model (奖励模型)**：训练一个“裁判模型”。人类标注员给 AI 的回答打分（A 比 B 好），裁判学会模仿人类的口味。
3.  **PPO 强化学习**：让 AI 自己生成回答，裁判打分。如果分高，就奖励参数更新；分低就惩罚。

**缺点**：太难了！RL (强化学习) 极其不稳定，训练经常崩，且流程复杂。

### 新王登基: DPO (Direct Preference Optimization)

2023 年出现的 **DPO** 正在取代 RLHF。
它的逻辑极简：**不需要奖励模型，也不需要强化学习循环。**

直接拿人类偏好数据（A 优于 B），用一个简单的分类损失函数去优化模型。
数学上证明了，它等价于 RLHF，但**稳定得多，省资源得多**。

> 💡 **工程建议**：如果你现在要自己做对齐，**首选 DPO**，别去碰 PPO。

---

## 5. 总结

1.  **架构决策**：先 Prompt，再 RAG，实在不行才 Fine-tuning。
2.  **SFT (指令微调)**：把“基座模型”变成“对话模型”的关键一步。
3.  **LoRA**：最实用的微调工具，轻量、高效、可插拔。
4.  **DPO**：取代 RLHF 的高效对齐手段。

掌握了如何训练模型，如果是把模型太大跑不动怎么办？
这就涉及到了下一章的内容：**如何把大象装进冰箱——模型量化与高效部署。**

---

**下一章**: [模型优化与部署](/blog/optimization-deployment)
